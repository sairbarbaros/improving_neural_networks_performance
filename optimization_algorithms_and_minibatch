import numpy as np
import math

#Firstly I will demonstrate how to divide the complete batch to the mini-batches.

def mini_batch_generator(X, Y, mini_batch_size=64):
    """

    :param X: Input data, (input size, num of examples)
    :param Y: True labels, (1, num of examples)
    :param mini_batch_size: size of mini batches, int
    :return: mini batches
    """

    m = X.shape[1]
    mini_batches = []

    #We mix them to get a random list of mini-batches.
    permutation = list(np.random.permutation(m))
    X_mix = X[:, permutation]
    Y_mix = Y[:, permutation].reshape((1, m))
    size = mini_batch_size
    total_mini_batches = math.floor(m/mini_batch_size)

    for i in range(0, total_mini_batches):

        mini_batch_X = X_mix[:, i * mini_batch_size: (i + 1) * mini_batch_size]
        mini_batch_Y = Y_mix[:, i * mini_batch_size: (i + 1) * mini_batch_size]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)

    if m % mini_batch_size != 0:

        mini_batch_X = X_mix[:, int(m/mini_batch_size)*mini_batch_size:]
        mini_batch_Y = Y_mix[:, int(m/mini_batch_size)*mini_batch_size:]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)

    return mini_batches

#Now, We are going to implement Momentum algorithm. It is an alternative algorithm to the gradient descent.

def initialize_v(parameters):
    """

    :param parameters: contains our parameters like W1, b1; is a dictionary
    :return: v-- stores current velocities
    """

    L = len(parameters) // 2
    v = {}

    for l in range(1, L+1):
        v["dW" + str(l)] = np.zeros((parameters["W" + str(l)].shape[0], parameters["W" + str(l)].shape[1]))
        v["db" + str(l)] = np.zeros((parameters["b" + str(l)].shape[0], parameters["b" + str(l)].shape[1]))

    return v

def momentum_parameter_updater(parameters, grads, v, beta=0.9, learning_rate=0.01):
    """

    :param parameters: contains our parameters, dictionary
    :param grads: contains our gradients, dictionary
    :param v: stores velocities
    :param beta: momentum algorithm hyperparameter
    :param learning_rate: learning hyperparameter
    :return: parameters, v--updated versions
    """

    L = len(parameters) // 2

    for l in range(1, L+1):

        v["dW" + str(l)] = beta * v["dW" + str(l)] + (1 - beta) * grads["dW" + str(l)]
        v["db" + str(l)] = beta * v["db" + str(l)] + (1 - beta) * grads["db" + str(l)]
        parameters["W" + str(l)] = parameters["W" + str(l)] - learning_rate * v["dW" + str(l)]
        parameters["b" + str(l)] = parameters["b" + str(l)] - learning_rate * v["db" + str(l)]

    return parameters, v

#To continue, It is time to implement Adam algorithm. You can think of it as a mix of momentum and RMS algorithms.

def initialize_adam(parameters):
    """

    :param parameters: contains our parameters, dictionary
    :return: v--exponentially weighted average of the gradients
             s--exponentially weighted average of the squared gradients
    """

    L = len(parameters)//2
    v = {}
    s = {}

    for l in range(1, L+1):

        v["dW" + str(l)] = np.zeros((parameters["W" + str(l)].shape[0], parameters["W" + str(l)].shape[1]))
        v["db" + str(l)] = np.zeros((parameters["b" + str(l)].shape[0], parameters["b" + str(l)].shape[1]))
        s["dW" + str(l)] = np.zeros((parameters["W" + str(l)].shape[0], parameters["W" + str(l)].shape[1]))
        s["db" + str(l)] = np.zeros((parameters["b" + str(l)].shape[0], parameters["b" + str(l)].shape[1]))

    return v, s

def adam_parameter_updater(parameters, grads, v, s, t, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8 ):
    """
    
    :param parameters: contains our parameters
    :param grads: contains our gradients
    :param v: moving average of the first gradient
    :param s: moving average of the squared gradient
    :param t: number of steps
    :param learning_rate: hyperparameter of parameter updating
    :param beta1: exponential decay hyperparameter for the momentum
    :param beta2: exponential decay hyperparameter for the RMS
    :param epsilon: hyperparameter for preventing division by zero
    :return: parameters, v, s, v_corrected, s_corrected
    """

    L = len(parameters)//2
    v_corrected = {}
    s_corrected = {}

    for l in range(1, L+1):

        v["dW" + str(l)] = beta1 * v["dW" + str(l)] + (1 - beta1) * grads["dW" + str(l)]
        v["db" + str(l)] = beta1 * v["db" + str(l)] + (1 - beta1) * grads["db" + str(l)]
        v_corrected["dW" + str(l)] = v["dW" + str(l)] / (1 - beta1 ** t)
        v_corrected["db" + str(l)] = v["db" + str(l)] / (1 - beta1 ** t)

        s["dW" + str(l)] = beta2 * s["dW" + str(l)] + (1 - beta2) * np.square(grads["dW" + str(l)])
        s["db" + str(l)] = beta2 * s["db" + str(l)] + (1 - beta2) * np.square(grads["db" + str(l)])
        s_corrected["dW" + str(l)] = s["dW" + str(l)] / (1 - beta2 ** t)
        s_corrected["db" + str(l)] = s["db" + str(l)] / (1 - beta2 ** t)

        parameters["W" + str(l)] = parameters["W" + str(l)] - learning_rate * v_corrected["dW" + str(l)] / (
                    np.sqrt(s_corrected["dW" + str(l)]) + epsilon)
        parameters["b" + str(l)] = parameters["b" + str(l)] - learning_rate * v_corrected["db" + str(l)] / (
                    np.sqrt(s_corrected["db" + str(l)]) + epsilon)

    return parameters, v, s, v_corrected, s_corrected
